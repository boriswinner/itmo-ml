{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Lab5_Timofeenko.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG8evyXUI_uA",
        "outputId": "15d5b5b3-8df1-4387-a63e-f4a997785fee"
      },
      "source": [
        "import os\r\n",
        "import urllib.request\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from pathlib import Path\r\n",
        "import nltk\r\n",
        "from nltk.text import TextCollection\r\n",
        "import string\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\r\n",
        "snowball_stemmer =  nltk.stem.SnowballStemmer('english')\r\n",
        "\r\n",
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", 'text.txt')\r\n",
        "text = Path('text.txt').read_text().lower()[1437:-18966] # Remove warnings and copyright notes\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkcDywKWjvr"
      },
      "source": [
        "\r\n",
        "def tokenize_and_clean_text_chunk(text):\r\n",
        "  tokens = nltk.word_tokenize(text)\r\n",
        "  tokens = [word for word in tokens if word.isalpha()]\r\n",
        "  table = str.maketrans('', '', string.punctuation)\r\n",
        "  tokens = [w.translate(table) for w in tokens]\r\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "  stop_words.append('alice') #Remove \"Alice\" from consideration\r\n",
        "  tokens = [w for w in tokens if not w in stop_words]\r\n",
        "  tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens] \r\n",
        "  return tokens\r\n",
        "\r\n",
        "chapters = text.split(\"chapter\")[1:]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZu8h4NgXW-_",
        "outputId": "7dbd1e3f-5ae2-4f09-c63a-ea7d5745ff30"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "for idx, chapter in enumerate(chapters):\r\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize_and_clean_text_chunk)\r\n",
        "  response = vectorizer.fit_transform([chapter])\r\n",
        "  terms = vectorizer.get_feature_names()\r\n",
        "  # sum tfidf frequency of each term through documents\r\n",
        "  sums = response.sum(axis=0)\r\n",
        "  # connecting term to its sums frequency\r\n",
        "  data = []\r\n",
        "  for col, term in enumerate(terms):\r\n",
        "      data.append( (term, sums[0,col] ))\r\n",
        "  ranking = pd.DataFrame(data, columns=['term','tf_idf'])\r\n",
        "  print(\"CHAPTER {}\".format(idx+1))\r\n",
        "  print(\"==========\")\r\n",
        "  print(ranking.sort_values('tf_idf', ascending=False)[:10])\r\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CHAPTER 1\n",
            "==========\n",
            "        term    tf_idf\n",
            "239   little  0.265331\n",
            "445      way  0.194576\n",
            "234     like  0.194576\n",
            "354      see  0.176887\n",
            "409    think  0.159199\n",
            "95      door  0.159199\n",
            "288      one  0.141510\n",
            "69     could  0.141510\n",
            "414     time  0.141510\n",
            "411  thought  0.141510\n",
            "\n",
            "\n",
            "CHAPTER 2\n",
            "==========\n",
            "       term    tf_idf\n",
            "212  little  0.267897\n",
            "235   mouse  0.267897\n",
            "317    said  0.200923\n",
            "146      go  0.184179\n",
            "72     dear  0.184179\n",
            "125    foot  0.167436\n",
            "398   thing  0.167436\n",
            "261      oh  0.150692\n",
            "438    went  0.150692\n",
            "238    must  0.150692\n",
            "\n",
            "\n",
            "CHAPTER 3\n",
            "==========\n",
            "      term    tf_idf\n",
            "317   said  0.575034\n",
            "241  mouse  0.355168\n",
            "105   dodo  0.202953\n",
            "195   know  0.186040\n",
            "260    one  0.135302\n",
            "348   soon  0.118389\n",
            "313  round  0.101477\n",
            "109    dry  0.101477\n",
            "213   long  0.101477\n",
            "217   lory  0.101477\n",
            "\n",
            "\n",
            "CHAPTER 4\n",
            "==========\n",
            "        term    tf_idf\n",
            "251   little  0.335098\n",
            "350   rabbit  0.218542\n",
            "373     said  0.203972\n",
            "307      one  0.203972\n",
            "34      bill  0.174834\n",
            "453  thought  0.131125\n",
            "159      get  0.131125\n",
            "190    heard  0.131125\n",
            "435     sure  0.131125\n",
            "349    quite  0.116556\n",
            "\n",
            "\n",
            "CHAPTER 5\n",
            "==========\n",
            "            term    tf_idf\n",
            "302         said  0.671373\n",
            "50   caterpillar  0.335687\n",
            "312      serpent  0.154932\n",
            "262       pigeon  0.154932\n",
            "427         well  0.129110\n",
            "200       little  0.129110\n",
            "223       minute  0.103288\n",
            "381        think  0.090377\n",
            "332         size  0.090377\n",
            "387         time  0.077466\n",
            "\n",
            "\n",
            "CHAPTER 6\n",
            "==========\n",
            "        term    tf_idf\n",
            "362     said  0.562562\n",
            "56       cat  0.287266\n",
            "238     like  0.191510\n",
            "103  duchess  0.167572\n",
            "242   little  0.167572\n",
            "143  footman  0.143633\n",
            "273     much  0.143633\n",
            "497    would  0.131663\n",
            "31      baby  0.131663\n",
            "485     went  0.119694\n",
            "\n",
            "\n",
            "CHAPTER 7\n",
            "==========\n",
            "         term    tf_idf\n",
            "295      said  0.614833\n",
            "149    hatter  0.349819\n",
            "78   dormouse  0.286216\n",
            "211     march  0.222612\n",
            "147      hare  0.222612\n",
            "362      time  0.180210\n",
            "353     thing  0.127207\n",
            "406      well  0.106006\n",
            "407      went  0.106006\n",
            "246       one  0.106006\n",
            "\n",
            "\n",
            "CHAPTER 8\n",
            "==========\n",
            "         term    tf_idf\n",
            "352      said  0.497853\n",
            "325     queen  0.438584\n",
            "192      head  0.189658\n",
            "216      king  0.165951\n",
            "414     three  0.130390\n",
            "62        cat  0.130390\n",
            "290       one  0.118536\n",
            "196  hedgehog  0.118536\n",
            "454      went  0.118536\n",
            "383   soldier  0.106683\n",
            "\n",
            "\n",
            "CHAPTER 9\n",
            "==========\n",
            "        term    tf_idf\n",
            "322     said  0.633999\n",
            "247     mock  0.289193\n",
            "415   turtle  0.289193\n",
            "166  gryphon  0.222456\n",
            "94   duchess  0.211333\n",
            "296    queen  0.155719\n",
            "446     went  0.144596\n",
            "259    never  0.122351\n",
            "220   little  0.111228\n",
            "326      say  0.100105\n",
            "\n",
            "\n",
            "CHAPTER 10\n",
            "==========\n",
            "          term    tf_idf\n",
            "263       said  0.519131\n",
            "141    gryphon  0.357624\n",
            "357     turtle  0.346087\n",
            "198       mock  0.323015\n",
            "387      would  0.173044\n",
            "181    lobster  0.149971\n",
            "67       dance  0.149971\n",
            "304       soup  0.126899\n",
            "17   beautiful  0.126899\n",
            "366      voice  0.115362\n",
            "\n",
            "\n",
            "CHAPTER 11\n",
            "==========\n",
            "         term    tf_idf\n",
            "269      said  0.538101\n",
            "163      king  0.368174\n",
            "141    hatter  0.283211\n",
            "61      court  0.212408\n",
            "82   dormouse  0.184087\n",
            "222       one  0.155766\n",
            "244     queen  0.127445\n",
            "375   witness  0.113284\n",
            "247    rabbit  0.113284\n",
            "332   thought  0.113284\n",
            "\n",
            "\n",
            "CHAPTER 12\n",
            "==========\n",
            "       term    tf_idf\n",
            "337    said  0.684932\n",
            "187    king  0.301370\n",
            "469   would  0.164384\n",
            "302   queen  0.136986\n",
            "210  little  0.136986\n",
            "183    jury  0.123288\n",
            "459   white  0.109589\n",
            "307  rabbit  0.109589\n",
            "160    head  0.109589\n",
            "267     one  0.109589\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCjsvmcehZP"
      },
      "source": [
        "Chapter 1: Little Alice thinks of doors and time\r\n",
        "\r\n",
        "Chapter 2: Little Alice sees mice and thinks\r\n",
        "\r\n",
        "Chapter 3: Little Alice knows a bird Dodo\r\n",
        "\r\n",
        "Chapter 4: Little Alice hears a rabbit\r\n",
        "\r\n",
        "Chapter 5: Little Alice thinks of a caterpillar\r\n",
        "\r\n",
        "Chapter 6: Little Alice likes cats\r\n",
        "\r\n",
        "Chapter 7: Little Alice matches with hatters\r\n",
        "\r\n",
        "Chapter 8: Little Alice goes to the queen\r\n",
        "\r\n",
        "Chapter 9: Little Alice mocks turtles\r\n",
        "\r\n",
        "Chapter 10: Little Alice dances with lobsters\r\n",
        "\r\n",
        "Chapter 11: Little Alice thoughts of hatters, dormice, rabbits, kings\r\n",
        "\r\n",
        "Chapter 12: Little Alice headed to jury, King and Queen with white rabbit\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFNUBfbMvD9_",
        "outputId": "505ca06b-cdbf-4772-8003-7443131ba1bb"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences = nltk.sent_tokenize(text)\r\n",
        "print(len(sentences))\r\n",
        "sentences_with_alice = []\r\n",
        "sentences_with_alice = [_ for _ in sentences if 'alice' in _]\r\n",
        "print(len(sentences_with_alice))\r\n",
        "all_verbs = []\r\n",
        "for sentence in sentences_with_alice:\r\n",
        "  tokens = tokenize_and_clean_text_chunk(sentence)\r\n",
        "  pos_tagged = nltk.pos_tag(tokens)\r\n",
        "  verbs = list(filter(lambda x:x[1]=='VB',pos_tagged))\r\n",
        "  verbs = [_[0] for _ in verbs]\r\n",
        "  for verb in verbs:\r\n",
        "    all_verbs.append(verb)\r\n",
        "print(all_verbs)\r\n",
        "\r\n",
        "def CountFrequency(my_list): \r\n",
        "  \r\n",
        "    # Creating an empty dictionary  \r\n",
        "    freq = {} \r\n",
        "    for item in my_list: \r\n",
        "        if (item in freq): \r\n",
        "            freq[item] += 1\r\n",
        "        else: \r\n",
        "            freq[item] = 1\r\n",
        "\r\n",
        "    freq = dict(sorted(freq.items(), key=lambda item: item[1], reverse=True))\r\n",
        "    for key, value in freq.items(): \r\n",
        "        print (\"{}: {}\".format(key, value)) \r\n",
        "\r\n",
        "CountFrequency(all_verbs) "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "970\n",
            "358\n",
            "['get', 'hot', 'worth', 'watch', 'see', 'like', 'think', 'see', 'thousand', 'see', 'knowledge', 'get', 'get', 'belong', 'get', 'go', 'little', 'knew', 'see', 'find', 'drink', 'say', 'wise', 'shrink', 'end', 'advise', 'get', 'feel', 'go', 'manage', 'go', 'say', 'glove', 'go', 'glove', 'mabel', 'go', 'tell', 'existence', 'come', 'find', 'use', 'right', 'wink', 'come', 'like', 'offended', 'let', 'name', 'say', 'go', 'get', 'go', 'say', 'know', 'allow', 'kept', 'catch', 'speak', 'prize', 'think', 'offended', 'go', 'wasting', 'try', 'please', 'come', 'see', 'glove', 'happen', 'miss', 'get', 'let', 'stop', 'glove', 'become', 'grow', 'get', 'take', 'hear', 'thought', 'know', 'come', 'burn', 'set', 'take', 'found', 'find', 'hungry', 'eat', 'keep', 'changed', 'feel', 'feel', 'tell', 'think', 'wait', 'tell', 'keep', 'remember', 'injure', 'suppose', 'grow', 'mushroom', 'make', 'see', 'let', 'see', 'neck', 'kept', 'sense', 'knock', 'let', 'get', 'sit', 'sneezing', 'tell', 'know', 'know', 'introduce', 'go', 'advantage', 'see', 'take', 'cook', 'take', 'hear', 'nurse', 'take', 'doubt', 'think', 'get', 'come', 'denied', 'go', 'know', 'cat', 'fun', 'say', 'say', 'get', 'say', 'remember', 'got', 'think', 'give', 'keep', 'keep', 'tell', 'like', 'take', 'take', 'take', 'go', 'let', 'move', 'draw', 'dormouse', 'go', 'see', 'ask', 'bear', 'call', 'tell', 'lie', 'remember', 'use', 'come', 'happen', 'help', 'happen', 'become', 'think', 'kiss', 'look', 'go', 'see', 'escape', 'think', 'ask', 'tell', 'remember', 'find', 'love', 'take', 'try', 'bite', 'seem', 'appear', 'appeared', 'say', 'make', 'feel', 'let', 'go', 'tell', 'go', 'quite', 'come', 'hear', 'break', 'help', 'come', 'ashamed', 'hold', 'speak', 'know', 'ten', 'lived', 'say', 'pretty', 'know', 'know', 'shrimp', 'told', 'keep', 'go', 'hear', 'tell', 'take', 'sugar', 'happen', 'go', 'come', 'think', 'like', 'come', 'pas', 'find', 'see', 'make', 'ask', 'stand', 'get', 'come', 'like', 'see', 'go', 'let', 'secret', 'tart', 'go', 'remember', 'wonderful', 'hear', 'keep', 'get']\n",
            "go: 19\n",
            "get: 13\n",
            "see: 12\n",
            "come: 11\n",
            "take: 10\n",
            "say: 9\n",
            "tell: 9\n",
            "think: 8\n",
            "know: 8\n",
            "let: 7\n",
            "keep: 6\n",
            "like: 5\n",
            "find: 5\n",
            "hear: 5\n",
            "remember: 5\n",
            "feel: 4\n",
            "glove: 4\n",
            "happen: 4\n",
            "make: 3\n",
            "ask: 3\n",
            "use: 2\n",
            "offended: 2\n",
            "kept: 2\n",
            "speak: 2\n",
            "try: 2\n",
            "become: 2\n",
            "grow: 2\n",
            "help: 2\n",
            "hot: 1\n",
            "worth: 1\n",
            "watch: 1\n",
            "thousand: 1\n",
            "knowledge: 1\n",
            "belong: 1\n",
            "little: 1\n",
            "knew: 1\n",
            "drink: 1\n",
            "wise: 1\n",
            "shrink: 1\n",
            "end: 1\n",
            "advise: 1\n",
            "manage: 1\n",
            "mabel: 1\n",
            "existence: 1\n",
            "right: 1\n",
            "wink: 1\n",
            "name: 1\n",
            "allow: 1\n",
            "catch: 1\n",
            "prize: 1\n",
            "wasting: 1\n",
            "please: 1\n",
            "miss: 1\n",
            "stop: 1\n",
            "thought: 1\n",
            "burn: 1\n",
            "set: 1\n",
            "found: 1\n",
            "hungry: 1\n",
            "eat: 1\n",
            "changed: 1\n",
            "wait: 1\n",
            "injure: 1\n",
            "suppose: 1\n",
            "mushroom: 1\n",
            "neck: 1\n",
            "sense: 1\n",
            "knock: 1\n",
            "sit: 1\n",
            "sneezing: 1\n",
            "introduce: 1\n",
            "advantage: 1\n",
            "cook: 1\n",
            "nurse: 1\n",
            "doubt: 1\n",
            "denied: 1\n",
            "cat: 1\n",
            "fun: 1\n",
            "got: 1\n",
            "give: 1\n",
            "move: 1\n",
            "draw: 1\n",
            "dormouse: 1\n",
            "bear: 1\n",
            "call: 1\n",
            "lie: 1\n",
            "kiss: 1\n",
            "look: 1\n",
            "escape: 1\n",
            "love: 1\n",
            "bite: 1\n",
            "seem: 1\n",
            "appear: 1\n",
            "appeared: 1\n",
            "quite: 1\n",
            "break: 1\n",
            "ashamed: 1\n",
            "hold: 1\n",
            "ten: 1\n",
            "lived: 1\n",
            "pretty: 1\n",
            "shrimp: 1\n",
            "told: 1\n",
            "sugar: 1\n",
            "pas: 1\n",
            "stand: 1\n",
            "secret: 1\n",
            "tart: 1\n",
            "wonderful: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Endex9yDBM"
      },
      "source": [
        ""
      ],
      "execution_count": 46,
      "outputs": []
    }
  ]
}