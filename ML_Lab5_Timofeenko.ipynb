{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Lab5_Timofeenko.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG8evyXUI_uA",
        "outputId": "14428d3f-8bb9-435d-f652-df0c084275d0"
      },
      "source": [
        "import os\r\n",
        "import urllib.request\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from pathlib import Path\r\n",
        "import nltk\r\n",
        "from nltk.text import TextCollection\r\n",
        "import string\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\r\n",
        "snowball_stemmer =  nltk.stem.SnowballStemmer('english')\r\n",
        "\r\n",
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", 'text.txt')\r\n",
        "text = Path('text.txt').read_text().lower()[1437:]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkcDywKWjvr"
      },
      "source": [
        "\r\n",
        "def tokenize_and_clean_text_chunk(text):\r\n",
        "  tokens = nltk.word_tokenize(text)\r\n",
        "  tokens = [word for word in tokens if word.isalpha()]\r\n",
        "  table = str.maketrans('', '', string.punctuation)\r\n",
        "  tokens = [w.translate(table) for w in tokens]\r\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "  tokens = [w for w in tokens if not w in stop_words]\r\n",
        "  tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens] \r\n",
        "  # tokens = [snowball_stemmer.stem(word) for word in tokens] \r\n",
        "  return tokens\r\n",
        "\r\n",
        "chapters = text.split(\"chapter\")[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZu8h4NgXW-_",
        "outputId": "4a843368-c88b-4c22-d28f-fe383e4b292f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "for idx, chapter in enumerate(chapters):\r\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize_and_clean_text_chunk)\r\n",
        "  response = vectorizer.fit_transform([chapter])\r\n",
        "  terms = vectorizer.get_feature_names()\r\n",
        "  # sum tfidf frequency of each term through documents\r\n",
        "  sums = response.sum(axis=0)\r\n",
        "  # connecting term to its sums frequency\r\n",
        "  data = []\r\n",
        "  for col, term in enumerate(terms):\r\n",
        "      data.append( (term, sums[0,col] ))\r\n",
        "  ranking = pd.DataFrame(data, columns=['term','tf_idf'])\r\n",
        "  print(\"CHAPTER {}\".format(idx+1))\r\n",
        "  print(\"==========\")\r\n",
        "  print(ranking.sort_values('tf_idf', ascending=False)[:11]) # +1 for Alice\r\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "CHAPTER 1\n",
            "==========\n",
            "       term    tf_idf\n",
            "8     alice  0.443830\n",
            "240  little  0.237766\n",
            "446     way  0.174362\n",
            "235    like  0.174362\n",
            "355     see  0.158511\n",
            "410   think  0.142660\n",
            "96     door  0.142660\n",
            "289     one  0.126809\n",
            "70    could  0.126809\n",
            "415    time  0.126809\n",
            "\n",
            "\n",
            "CHAPTER 2\n",
            "==========\n",
            "        term    tf_idf\n",
            "7      alice  0.386126\n",
            "213   little  0.247121\n",
            "236    mouse  0.247121\n",
            "318     said  0.185341\n",
            "73      dear  0.169896\n",
            "147       go  0.169896\n",
            "126     foot  0.154451\n",
            "399    thing  0.154451\n",
            "239     must  0.139005\n",
            "403  thought  0.139005\n",
            "\n",
            "\n",
            "CHAPTER 3\n",
            "==========\n",
            "      term    tf_idf\n",
            "318   said  0.535915\n",
            "13   alice  0.362531\n",
            "242  mouse  0.331006\n",
            "106   dodo  0.189146\n",
            "196   know  0.173384\n",
            "261    one  0.126098\n",
            "349   soon  0.110335\n",
            "47    bird  0.094573\n",
            "314  round  0.094573\n",
            "372  thing  0.094573\n",
            "\n",
            "\n",
            "CHAPTER 4\n",
            "==========\n",
            "        term    tf_idf\n",
            "4      alice  0.411617\n",
            "252   little  0.305393\n",
            "351   rabbit  0.199170\n",
            "308      one  0.185892\n",
            "374     said  0.185892\n",
            "35      bill  0.159336\n",
            "454  thought  0.119502\n",
            "191    heard  0.119502\n",
            "436     sure  0.119502\n",
            "160      get  0.119502\n",
            "\n",
            "\n",
            "CHAPTER 5\n",
            "==========\n",
            "            term    tf_idf\n",
            "303         said  0.611807\n",
            "8          alice  0.411793\n",
            "51   caterpillar  0.305904\n",
            "313      serpent  0.141186\n",
            "263       pigeon  0.141186\n",
            "201       little  0.117655\n",
            "428         well  0.117655\n",
            "224       minute  0.094124\n",
            "333         size  0.082359\n",
            "382        think  0.082359\n",
            "\n",
            "\n",
            "CHAPTER 6\n",
            "==========\n",
            "        term    tf_idf\n",
            "363     said  0.500198\n",
            "8      alice  0.457628\n",
            "57       cat  0.255420\n",
            "239     like  0.170280\n",
            "243   little  0.148995\n",
            "104  duchess  0.148995\n",
            "274     much  0.127710\n",
            "144  footman  0.127710\n",
            "32      baby  0.117068\n",
            "498    would  0.117068\n",
            "\n",
            "\n",
            "CHAPTER 7\n",
            "==========\n",
            "         term    tf_idf\n",
            "296      said  0.540853\n",
            "6       alice  0.475577\n",
            "150    hatter  0.307727\n",
            "79   dormouse  0.251776\n",
            "212     march  0.195826\n",
            "148      hare  0.195826\n",
            "363      time  0.158526\n",
            "354     thing  0.111901\n",
            "407      well  0.093250\n",
            "408      went  0.093250\n",
            "\n",
            "\n",
            "CHAPTER 8\n",
            "==========\n",
            "         term    tf_idf\n",
            "353      said  0.451900\n",
            "7       alice  0.419622\n",
            "326     queen  0.398103\n",
            "193      head  0.172153\n",
            "217      king  0.150633\n",
            "415     three  0.118355\n",
            "63        cat  0.118355\n",
            "291       one  0.107595\n",
            "455      went  0.107595\n",
            "197  hedgehog  0.107595\n",
            "\n",
            "\n",
            "CHAPTER 9\n",
            "==========\n",
            "        term    tf_idf\n",
            "323     said  0.548813\n",
            "7      alice  0.500672\n",
            "248     mock  0.250336\n",
            "416   turtle  0.250336\n",
            "167  gryphon  0.192566\n",
            "95   duchess  0.182938\n",
            "297    queen  0.134796\n",
            "447     went  0.125168\n",
            "260    never  0.105911\n",
            "221   little  0.096283\n",
            "\n",
            "\n",
            "CHAPTER 10\n",
            "==========\n",
            "          term    tf_idf\n",
            "264       said  0.490582\n",
            "142    gryphon  0.337956\n",
            "358     turtle  0.327054\n",
            "6        alice  0.327054\n",
            "199       mock  0.305251\n",
            "388      would  0.163527\n",
            "182    lobster  0.141724\n",
            "68       dance  0.141724\n",
            "305       soup  0.119920\n",
            "18   beautiful  0.119920\n",
            "\n",
            "\n",
            "CHAPTER 11\n",
            "==========\n",
            "         term    tf_idf\n",
            "270      said  0.524800\n",
            "164      king  0.359074\n",
            "142    hatter  0.276210\n",
            "5       alice  0.220968\n",
            "62      court  0.207158\n",
            "83   dormouse  0.179537\n",
            "223       one  0.151916\n",
            "245     queen  0.124295\n",
            "333   thought  0.110484\n",
            "248    rabbit  0.110484\n",
            "\n",
            "\n",
            "CHAPTER 12\n",
            "==========\n",
            "           term    tf_idf\n",
            "659     project  0.493727\n",
            "957        work  0.453067\n",
            "751        said  0.290428\n",
            "353   gutenberg  0.162640\n",
            "247  electronic  0.156831\n",
            "27        alice  0.145214\n",
            "311  foundation  0.145214\n",
            "823       state  0.133597\n",
            "430        king  0.127788\n",
            "849        term  0.116171\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCjsvmcehZP"
      },
      "source": [
        "Chapter 1: Little Alice thinks of doors and time\r\n",
        "\r\n",
        "Chapter 2: Little Alice sees mice and thinks\r\n",
        "\r\n",
        "Chapter 3: Little Alice knows a bird Dodo\r\n",
        "\r\n",
        "Chapter 4: Little Alice hears a rabbit\r\n",
        "\r\n",
        "Chapter 5: Little Alice thinks of a caterpillar\r\n",
        "\r\n",
        "Chapter 6: Little Alice likes cats\r\n",
        "\r\n",
        "Chapter 7: Little Alice matches with hatters\r\n",
        "\r\n",
        "Chapter 8: Little Alice goes to the queen\r\n",
        "\r\n",
        "Chapter 9: Little Alice mocks turtles\r\n",
        "\r\n",
        "Chapter 10: Little Alice dances with lobsters\r\n",
        "\r\n",
        "Chapter 11: Little Alice thoughts of hatters, dormice, rabbits, kings\r\n",
        "\r\n",
        "Chapter 12: Little Alice works on project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFNUBfbMvD9_",
        "outputId": "7f674ad3-2579-4c5d-efd4-8c85db7c7345"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "sentences = nltk.sent_tokenize(text)\r\n",
        "print(len(sentences))\r\n",
        "sentences_with_alice = []\r\n",
        "sentences_with_alice = [_ for _ in sentences if 'alice' in _]\r\n",
        "print(len(sentences_with_alice))\r\n",
        "all_verbs = []\r\n",
        "for sentence in sentences_with_alice:\r\n",
        "  tokens = tokenize_and_clean_text_chunk(sentence)\r\n",
        "  pos_tagged = nltk.pos_tag(tokens)\r\n",
        "  verbs = list(filter(lambda x:x[1]=='VB',pos_tagged))\r\n",
        "  verbs = [_[0] for _ in verbs]\r\n",
        "  for verb in verbs:\r\n",
        "    all_verbs.append(verb)\r\n",
        "print(all_verbs)\r\n",
        "\r\n",
        "def CountFrequency(my_list): \r\n",
        "  \r\n",
        "    # Creating an empty dictionary  \r\n",
        "    freq = {} \r\n",
        "    for item in my_list: \r\n",
        "        if (item in freq): \r\n",
        "            freq[item] += 1\r\n",
        "        else: \r\n",
        "            freq[item] = 1\r\n",
        "\r\n",
        "    freq = dict(sorted(freq.items(), key=lambda item: item[1], reverse=True))\r\n",
        "    for key, value in freq.items(): \r\n",
        "        print (\"{}: {}\".format(key, value)) \r\n",
        "\r\n",
        "CountFrequency(all_verbs) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "1080\n",
            "359\n",
            "['get', 'hot', 'worth', 'watch', 'see', 'like', 'think', 'see', 'thousand', 'knowledge', 'get', 'get', 'belong', 'get', 'go', 'little', 'knew', 'see', 'find', 'drink', 'say', 'wise', 'shrink', 'end', 'advise', 'get', 'feel', 'go', 'manage', 'walk', 'go', 'say', 'glove', 'go', 'glove', 'mabel', 'go', 'tell', 'existence', 'come', 'find', 'use', 'right', 'wink', 'come', 'like', 'offended', 'let', 'name', 'say', 'go', 'get', 'go', 'find', 'say', 'know', 'allow', 'catch', 'speak', 'prize', 'think', 'offended', 'go', 'wasting', 'try', 'please', 'come', 'see', 'glove', 'happen', 'miss', 'get', 'stop', 'glove', 'grow', 'get', 'take', 'hear', 'thought', 'know', 'come', 'burn', 'set', 'take', 'found', 'find', 'hungry', 'eat', 'keep', 'changed', 'feel', 'feel', 'tell', 'think', 'wait', 'tell', 'remember', 'keep', 'remember', 'injure', 'suppose', 'grow', 'mushroom', 'make', 'see', 'let', 'see', 'neck', 'kept', 'sense', 'knock', 'let', 'get', 'sit', 'sneezing', 'tell', 'know', 'know', 'introduce', 'go', 'advantage', 'see', 'take', 'cook', 'take', 'hear', 'nurse', 'take', 'doubt', 'come', 'get', 'denied', 'go', 'know', 'fun', 'say', 'say', 'get', 'say', 'remember', 'got', 'think', 'keep', 'keep', 'tell', 'like', 'take', 'take', 'take', 'go', 'let', 'move', 'draw', 'dormouse', 'go', 'see', 'ask', 'bear', 'call', 'tell', 'lie', 'remember', 'use', 'come', 'happen', 'help', 'happen', 'become', 'think', 'kiss', 'look', 'go', 'see', 'escape', 'think', 'ask', 'tell', 'remember', 'find', 'love', 'take', 'try', 'bite', 'seem', 'appear', 'appeared', 'say', 'make', 'feel', 'let', 'go', 'tell', 'go', 'quite', 'come', 'hear', 'break', 'help', 'come', 'ashamed', 'hold', 'speak', 'know', 'ten', 'lived', 'pretty', 'know', 'know', 'shrimp', 'told', 'keep', 'go', 'hear', 'tell', 'take', 'sugar', 'happen', 'go', 'come', 'think', 'like', 'come', 'pas', 'find', 'see', 'make', 'ask', 'stand', 'get', 'come', 'like', 'see', 'go', 'let', 'secret', 'tart', 'go', 'remember', 'wonderful', 'hear', 'keep', 'get', 'replace']\n",
            "go: 19\n",
            "get: 13\n",
            "see: 11\n",
            "come: 11\n",
            "take: 10\n",
            "tell: 9\n",
            "say: 8\n",
            "know: 8\n",
            "think: 7\n",
            "find: 6\n",
            "let: 6\n",
            "keep: 6\n",
            "remember: 6\n",
            "like: 5\n",
            "hear: 5\n",
            "feel: 4\n",
            "glove: 4\n",
            "happen: 4\n",
            "make: 3\n",
            "ask: 3\n",
            "use: 2\n",
            "offended: 2\n",
            "speak: 2\n",
            "try: 2\n",
            "grow: 2\n",
            "help: 2\n",
            "hot: 1\n",
            "worth: 1\n",
            "watch: 1\n",
            "thousand: 1\n",
            "knowledge: 1\n",
            "belong: 1\n",
            "little: 1\n",
            "knew: 1\n",
            "drink: 1\n",
            "wise: 1\n",
            "shrink: 1\n",
            "end: 1\n",
            "advise: 1\n",
            "manage: 1\n",
            "walk: 1\n",
            "mabel: 1\n",
            "existence: 1\n",
            "right: 1\n",
            "wink: 1\n",
            "name: 1\n",
            "allow: 1\n",
            "catch: 1\n",
            "prize: 1\n",
            "wasting: 1\n",
            "please: 1\n",
            "miss: 1\n",
            "stop: 1\n",
            "thought: 1\n",
            "burn: 1\n",
            "set: 1\n",
            "found: 1\n",
            "hungry: 1\n",
            "eat: 1\n",
            "changed: 1\n",
            "wait: 1\n",
            "injure: 1\n",
            "suppose: 1\n",
            "mushroom: 1\n",
            "neck: 1\n",
            "kept: 1\n",
            "sense: 1\n",
            "knock: 1\n",
            "sit: 1\n",
            "sneezing: 1\n",
            "introduce: 1\n",
            "advantage: 1\n",
            "cook: 1\n",
            "nurse: 1\n",
            "doubt: 1\n",
            "denied: 1\n",
            "fun: 1\n",
            "got: 1\n",
            "move: 1\n",
            "draw: 1\n",
            "dormouse: 1\n",
            "bear: 1\n",
            "call: 1\n",
            "lie: 1\n",
            "become: 1\n",
            "kiss: 1\n",
            "look: 1\n",
            "escape: 1\n",
            "love: 1\n",
            "bite: 1\n",
            "seem: 1\n",
            "appear: 1\n",
            "appeared: 1\n",
            "quite: 1\n",
            "break: 1\n",
            "ashamed: 1\n",
            "hold: 1\n",
            "ten: 1\n",
            "lived: 1\n",
            "pretty: 1\n",
            "shrimp: 1\n",
            "told: 1\n",
            "sugar: 1\n",
            "pas: 1\n",
            "stand: 1\n",
            "secret: 1\n",
            "tart: 1\n",
            "wonderful: 1\n",
            "replace: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Endex9yDBM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}